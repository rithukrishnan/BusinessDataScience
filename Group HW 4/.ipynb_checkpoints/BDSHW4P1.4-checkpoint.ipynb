{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDS Homework 4 - Problem 2 - CIFAR dataset\n",
    "### &\n",
    "### Problem 4 - CIFAR Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data':           a0     a1     a2     a3     a4     a5     a6     a7     a8     a9  \\\n",
       " 0       59.0   43.0   50.0   68.0   98.0  119.0  139.0  145.0  149.0  149.0   \n",
       " 1      154.0  126.0  105.0  102.0  125.0  155.0  172.0  180.0  142.0  111.0   \n",
       " 2      255.0  253.0  253.0  253.0  253.0  253.0  253.0  253.0  253.0  253.0   \n",
       " 3       28.0   37.0   38.0   42.0   44.0   40.0   40.0   24.0   32.0   43.0   \n",
       " 4      170.0  168.0  177.0  183.0  181.0  177.0  181.0  184.0  189.0  189.0   \n",
       " ...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       " 19995   76.0   76.0   77.0   76.0   75.0   76.0   76.0   76.0   76.0   78.0   \n",
       " 19996   81.0   91.0   98.0  106.0  108.0  110.0   80.0   84.0   88.0   90.0   \n",
       " 19997   20.0   19.0   15.0   15.0   14.0   13.0   12.0   11.0   10.0    9.0   \n",
       " 19998   25.0   15.0   23.0   17.0   23.0   51.0   74.0   91.0  114.0  137.0   \n",
       " 19999   73.0   98.0   99.0   77.0   59.0  146.0  214.0  176.0  125.0  218.0   \n",
       " \n",
       "        ...  a3062  a3063  a3064  a3065  a3066  a3067  a3068  a3069  a3070  \\\n",
       " 0      ...   59.0   58.0   65.0   59.0   46.0   57.0  104.0  140.0   84.0   \n",
       " 1      ...   22.0   42.0   67.0  101.0  122.0  133.0  136.0  139.0  142.0   \n",
       " 2      ...   78.0   83.0   80.0   69.0   66.0   72.0   79.0   83.0   83.0   \n",
       " 3      ...   53.0   39.0   59.0   42.0   44.0   48.0   38.0   28.0   37.0   \n",
       " 4      ...   92.0   88.0   85.0   82.0   83.0   79.0   78.0   82.0   78.0   \n",
       " ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       " 19995  ...  228.0  185.0  177.0  223.0  239.0  239.0  235.0  236.0  234.0   \n",
       " 19996  ...  126.0  107.0  143.0  155.0  156.0  160.0  173.0  129.0  147.0   \n",
       " 19997  ...  114.0  112.0   68.0   50.0   52.0   52.0   51.0   50.0   53.0   \n",
       " 19998  ...   87.0   84.0   83.0   84.0   79.0   78.0   78.0   80.0   81.0   \n",
       " 19999  ...   84.0   89.0   88.0   85.0   93.0   93.0   90.0   94.0   58.0   \n",
       " \n",
       "        a3071  \n",
       " 0       72.0  \n",
       " 1      144.0  \n",
       " 2       84.0  \n",
       " 3       46.0  \n",
       " 4       80.0  \n",
       " ...      ...  \n",
       " 19995  236.0  \n",
       " 19996  160.0  \n",
       " 19997   47.0  \n",
       " 19998   80.0  \n",
       " 19999   26.0  \n",
       " \n",
       " [20000 rows x 3072 columns],\n",
       " 'target': 0        6\n",
       " 1        9\n",
       " 2        9\n",
       " 3        4\n",
       " 4        1\n",
       "         ..\n",
       " 19995    8\n",
       " 19996    3\n",
       " 19997    5\n",
       " 19998    1\n",
       " 19999    7\n",
       " Name: class, Length: 20000, dtype: category\n",
       " Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9'],\n",
       " 'frame':           a0     a1     a2     a3     a4     a5     a6     a7     a8     a9  \\\n",
       " 0       59.0   43.0   50.0   68.0   98.0  119.0  139.0  145.0  149.0  149.0   \n",
       " 1      154.0  126.0  105.0  102.0  125.0  155.0  172.0  180.0  142.0  111.0   \n",
       " 2      255.0  253.0  253.0  253.0  253.0  253.0  253.0  253.0  253.0  253.0   \n",
       " 3       28.0   37.0   38.0   42.0   44.0   40.0   40.0   24.0   32.0   43.0   \n",
       " 4      170.0  168.0  177.0  183.0  181.0  177.0  181.0  184.0  189.0  189.0   \n",
       " ...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       " 19995   76.0   76.0   77.0   76.0   75.0   76.0   76.0   76.0   76.0   78.0   \n",
       " 19996   81.0   91.0   98.0  106.0  108.0  110.0   80.0   84.0   88.0   90.0   \n",
       " 19997   20.0   19.0   15.0   15.0   14.0   13.0   12.0   11.0   10.0    9.0   \n",
       " 19998   25.0   15.0   23.0   17.0   23.0   51.0   74.0   91.0  114.0  137.0   \n",
       " 19999   73.0   98.0   99.0   77.0   59.0  146.0  214.0  176.0  125.0  218.0   \n",
       " \n",
       "        ...  a3063  a3064  a3065  a3066  a3067  a3068  a3069  a3070  a3071  \\\n",
       " 0      ...   58.0   65.0   59.0   46.0   57.0  104.0  140.0   84.0   72.0   \n",
       " 1      ...   42.0   67.0  101.0  122.0  133.0  136.0  139.0  142.0  144.0   \n",
       " 2      ...   83.0   80.0   69.0   66.0   72.0   79.0   83.0   83.0   84.0   \n",
       " 3      ...   39.0   59.0   42.0   44.0   48.0   38.0   28.0   37.0   46.0   \n",
       " 4      ...   88.0   85.0   82.0   83.0   79.0   78.0   82.0   78.0   80.0   \n",
       " ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       " 19995  ...  185.0  177.0  223.0  239.0  239.0  235.0  236.0  234.0  236.0   \n",
       " 19996  ...  107.0  143.0  155.0  156.0  160.0  173.0  129.0  147.0  160.0   \n",
       " 19997  ...  112.0   68.0   50.0   52.0   52.0   51.0   50.0   53.0   47.0   \n",
       " 19998  ...   84.0   83.0   84.0   79.0   78.0   78.0   80.0   81.0   80.0   \n",
       " 19999  ...   89.0   88.0   85.0   93.0   93.0   90.0   94.0   58.0   26.0   \n",
       " \n",
       "        class  \n",
       " 0          6  \n",
       " 1          9  \n",
       " 2          9  \n",
       " 3          4  \n",
       " 4          1  \n",
       " ...      ...  \n",
       " 19995      8  \n",
       " 19996      3  \n",
       " 19997      5  \n",
       " 19998      1  \n",
       " 19999      7  \n",
       " \n",
       " [20000 rows x 3073 columns],\n",
       " 'categories': None,\n",
       " 'feature_names': ['a0',\n",
       "  'a1',\n",
       "  'a2',\n",
       "  'a3',\n",
       "  'a4',\n",
       "  'a5',\n",
       "  'a6',\n",
       "  'a7',\n",
       "  'a8',\n",
       "  'a9',\n",
       "  'a10',\n",
       "  'a11',\n",
       "  'a12',\n",
       "  'a13',\n",
       "  'a14',\n",
       "  'a15',\n",
       "  'a16',\n",
       "  'a17',\n",
       "  'a18',\n",
       "  'a19',\n",
       "  'a20',\n",
       "  'a21',\n",
       "  'a22',\n",
       "  'a23',\n",
       "  'a24',\n",
       "  'a25',\n",
       "  'a26',\n",
       "  'a27',\n",
       "  'a28',\n",
       "  'a29',\n",
       "  'a30',\n",
       "  'a31',\n",
       "  'a32',\n",
       "  'a33',\n",
       "  'a34',\n",
       "  'a35',\n",
       "  'a36',\n",
       "  'a37',\n",
       "  'a38',\n",
       "  'a39',\n",
       "  'a40',\n",
       "  'a41',\n",
       "  'a42',\n",
       "  'a43',\n",
       "  'a44',\n",
       "  'a45',\n",
       "  'a46',\n",
       "  'a47',\n",
       "  'a48',\n",
       "  'a49',\n",
       "  'a50',\n",
       "  'a51',\n",
       "  'a52',\n",
       "  'a53',\n",
       "  'a54',\n",
       "  'a55',\n",
       "  'a56',\n",
       "  'a57',\n",
       "  'a58',\n",
       "  'a59',\n",
       "  'a60',\n",
       "  'a61',\n",
       "  'a62',\n",
       "  'a63',\n",
       "  'a64',\n",
       "  'a65',\n",
       "  'a66',\n",
       "  'a67',\n",
       "  'a68',\n",
       "  'a69',\n",
       "  'a70',\n",
       "  'a71',\n",
       "  'a72',\n",
       "  'a73',\n",
       "  'a74',\n",
       "  'a75',\n",
       "  'a76',\n",
       "  'a77',\n",
       "  'a78',\n",
       "  'a79',\n",
       "  'a80',\n",
       "  'a81',\n",
       "  'a82',\n",
       "  'a83',\n",
       "  'a84',\n",
       "  'a85',\n",
       "  'a86',\n",
       "  'a87',\n",
       "  'a88',\n",
       "  'a89',\n",
       "  'a90',\n",
       "  'a91',\n",
       "  'a92',\n",
       "  'a93',\n",
       "  'a94',\n",
       "  'a95',\n",
       "  'a96',\n",
       "  'a97',\n",
       "  'a98',\n",
       "  'a99',\n",
       "  'a100',\n",
       "  'a101',\n",
       "  'a102',\n",
       "  'a103',\n",
       "  'a104',\n",
       "  'a105',\n",
       "  'a106',\n",
       "  'a107',\n",
       "  'a108',\n",
       "  'a109',\n",
       "  'a110',\n",
       "  'a111',\n",
       "  'a112',\n",
       "  'a113',\n",
       "  'a114',\n",
       "  'a115',\n",
       "  'a116',\n",
       "  'a117',\n",
       "  'a118',\n",
       "  'a119',\n",
       "  'a120',\n",
       "  'a121',\n",
       "  'a122',\n",
       "  'a123',\n",
       "  'a124',\n",
       "  'a125',\n",
       "  'a126',\n",
       "  'a127',\n",
       "  'a128',\n",
       "  'a129',\n",
       "  'a130',\n",
       "  'a131',\n",
       "  'a132',\n",
       "  'a133',\n",
       "  'a134',\n",
       "  'a135',\n",
       "  'a136',\n",
       "  'a137',\n",
       "  'a138',\n",
       "  'a139',\n",
       "  'a140',\n",
       "  'a141',\n",
       "  'a142',\n",
       "  'a143',\n",
       "  'a144',\n",
       "  'a145',\n",
       "  'a146',\n",
       "  'a147',\n",
       "  'a148',\n",
       "  'a149',\n",
       "  'a150',\n",
       "  'a151',\n",
       "  'a152',\n",
       "  'a153',\n",
       "  'a154',\n",
       "  'a155',\n",
       "  'a156',\n",
       "  'a157',\n",
       "  'a158',\n",
       "  'a159',\n",
       "  'a160',\n",
       "  'a161',\n",
       "  'a162',\n",
       "  'a163',\n",
       "  'a164',\n",
       "  'a165',\n",
       "  'a166',\n",
       "  'a167',\n",
       "  'a168',\n",
       "  'a169',\n",
       "  'a170',\n",
       "  'a171',\n",
       "  'a172',\n",
       "  'a173',\n",
       "  'a174',\n",
       "  'a175',\n",
       "  'a176',\n",
       "  'a177',\n",
       "  'a178',\n",
       "  'a179',\n",
       "  'a180',\n",
       "  'a181',\n",
       "  'a182',\n",
       "  'a183',\n",
       "  'a184',\n",
       "  'a185',\n",
       "  'a186',\n",
       "  'a187',\n",
       "  'a188',\n",
       "  'a189',\n",
       "  'a190',\n",
       "  'a191',\n",
       "  'a192',\n",
       "  'a193',\n",
       "  'a194',\n",
       "  'a195',\n",
       "  'a196',\n",
       "  'a197',\n",
       "  'a198',\n",
       "  'a199',\n",
       "  'a200',\n",
       "  'a201',\n",
       "  'a202',\n",
       "  'a203',\n",
       "  'a204',\n",
       "  'a205',\n",
       "  'a206',\n",
       "  'a207',\n",
       "  'a208',\n",
       "  'a209',\n",
       "  'a210',\n",
       "  'a211',\n",
       "  'a212',\n",
       "  'a213',\n",
       "  'a214',\n",
       "  'a215',\n",
       "  'a216',\n",
       "  'a217',\n",
       "  'a218',\n",
       "  'a219',\n",
       "  'a220',\n",
       "  'a221',\n",
       "  'a222',\n",
       "  'a223',\n",
       "  'a224',\n",
       "  'a225',\n",
       "  'a226',\n",
       "  'a227',\n",
       "  'a228',\n",
       "  'a229',\n",
       "  'a230',\n",
       "  'a231',\n",
       "  'a232',\n",
       "  'a233',\n",
       "  'a234',\n",
       "  'a235',\n",
       "  'a236',\n",
       "  'a237',\n",
       "  'a238',\n",
       "  'a239',\n",
       "  'a240',\n",
       "  'a241',\n",
       "  'a242',\n",
       "  'a243',\n",
       "  'a244',\n",
       "  'a245',\n",
       "  'a246',\n",
       "  'a247',\n",
       "  'a248',\n",
       "  'a249',\n",
       "  'a250',\n",
       "  'a251',\n",
       "  'a252',\n",
       "  'a253',\n",
       "  'a254',\n",
       "  'a255',\n",
       "  'a256',\n",
       "  'a257',\n",
       "  'a258',\n",
       "  'a259',\n",
       "  'a260',\n",
       "  'a261',\n",
       "  'a262',\n",
       "  'a263',\n",
       "  'a264',\n",
       "  'a265',\n",
       "  'a266',\n",
       "  'a267',\n",
       "  'a268',\n",
       "  'a269',\n",
       "  'a270',\n",
       "  'a271',\n",
       "  'a272',\n",
       "  'a273',\n",
       "  'a274',\n",
       "  'a275',\n",
       "  'a276',\n",
       "  'a277',\n",
       "  'a278',\n",
       "  'a279',\n",
       "  'a280',\n",
       "  'a281',\n",
       "  'a282',\n",
       "  'a283',\n",
       "  'a284',\n",
       "  'a285',\n",
       "  'a286',\n",
       "  'a287',\n",
       "  'a288',\n",
       "  'a289',\n",
       "  'a290',\n",
       "  'a291',\n",
       "  'a292',\n",
       "  'a293',\n",
       "  'a294',\n",
       "  'a295',\n",
       "  'a296',\n",
       "  'a297',\n",
       "  'a298',\n",
       "  'a299',\n",
       "  'a300',\n",
       "  'a301',\n",
       "  'a302',\n",
       "  'a303',\n",
       "  'a304',\n",
       "  'a305',\n",
       "  'a306',\n",
       "  'a307',\n",
       "  'a308',\n",
       "  'a309',\n",
       "  'a310',\n",
       "  'a311',\n",
       "  'a312',\n",
       "  'a313',\n",
       "  'a314',\n",
       "  'a315',\n",
       "  'a316',\n",
       "  'a317',\n",
       "  'a318',\n",
       "  'a319',\n",
       "  'a320',\n",
       "  'a321',\n",
       "  'a322',\n",
       "  'a323',\n",
       "  'a324',\n",
       "  'a325',\n",
       "  'a326',\n",
       "  'a327',\n",
       "  'a328',\n",
       "  'a329',\n",
       "  'a330',\n",
       "  'a331',\n",
       "  'a332',\n",
       "  'a333',\n",
       "  'a334',\n",
       "  'a335',\n",
       "  'a336',\n",
       "  'a337',\n",
       "  'a338',\n",
       "  'a339',\n",
       "  'a340',\n",
       "  'a341',\n",
       "  'a342',\n",
       "  'a343',\n",
       "  'a344',\n",
       "  'a345',\n",
       "  'a346',\n",
       "  'a347',\n",
       "  'a348',\n",
       "  'a349',\n",
       "  'a350',\n",
       "  'a351',\n",
       "  'a352',\n",
       "  'a353',\n",
       "  'a354',\n",
       "  'a355',\n",
       "  'a356',\n",
       "  'a357',\n",
       "  'a358',\n",
       "  'a359',\n",
       "  'a360',\n",
       "  'a361',\n",
       "  'a362',\n",
       "  'a363',\n",
       "  'a364',\n",
       "  'a365',\n",
       "  'a366',\n",
       "  'a367',\n",
       "  'a368',\n",
       "  'a369',\n",
       "  'a370',\n",
       "  'a371',\n",
       "  'a372',\n",
       "  'a373',\n",
       "  'a374',\n",
       "  'a375',\n",
       "  'a376',\n",
       "  'a377',\n",
       "  'a378',\n",
       "  'a379',\n",
       "  'a380',\n",
       "  'a381',\n",
       "  'a382',\n",
       "  'a383',\n",
       "  'a384',\n",
       "  'a385',\n",
       "  'a386',\n",
       "  'a387',\n",
       "  'a388',\n",
       "  'a389',\n",
       "  'a390',\n",
       "  'a391',\n",
       "  'a392',\n",
       "  'a393',\n",
       "  'a394',\n",
       "  'a395',\n",
       "  'a396',\n",
       "  'a397',\n",
       "  'a398',\n",
       "  'a399',\n",
       "  'a400',\n",
       "  'a401',\n",
       "  'a402',\n",
       "  'a403',\n",
       "  'a404',\n",
       "  'a405',\n",
       "  'a406',\n",
       "  'a407',\n",
       "  'a408',\n",
       "  'a409',\n",
       "  'a410',\n",
       "  'a411',\n",
       "  'a412',\n",
       "  'a413',\n",
       "  'a414',\n",
       "  'a415',\n",
       "  'a416',\n",
       "  'a417',\n",
       "  'a418',\n",
       "  'a419',\n",
       "  'a420',\n",
       "  'a421',\n",
       "  'a422',\n",
       "  'a423',\n",
       "  'a424',\n",
       "  'a425',\n",
       "  'a426',\n",
       "  'a427',\n",
       "  'a428',\n",
       "  'a429',\n",
       "  'a430',\n",
       "  'a431',\n",
       "  'a432',\n",
       "  'a433',\n",
       "  'a434',\n",
       "  'a435',\n",
       "  'a436',\n",
       "  'a437',\n",
       "  'a438',\n",
       "  'a439',\n",
       "  'a440',\n",
       "  'a441',\n",
       "  'a442',\n",
       "  'a443',\n",
       "  'a444',\n",
       "  'a445',\n",
       "  'a446',\n",
       "  'a447',\n",
       "  'a448',\n",
       "  'a449',\n",
       "  'a450',\n",
       "  'a451',\n",
       "  'a452',\n",
       "  'a453',\n",
       "  'a454',\n",
       "  'a455',\n",
       "  'a456',\n",
       "  'a457',\n",
       "  'a458',\n",
       "  'a459',\n",
       "  'a460',\n",
       "  'a461',\n",
       "  'a462',\n",
       "  'a463',\n",
       "  'a464',\n",
       "  'a465',\n",
       "  'a466',\n",
       "  'a467',\n",
       "  'a468',\n",
       "  'a469',\n",
       "  'a470',\n",
       "  'a471',\n",
       "  'a472',\n",
       "  'a473',\n",
       "  'a474',\n",
       "  'a475',\n",
       "  'a476',\n",
       "  'a477',\n",
       "  'a478',\n",
       "  'a479',\n",
       "  'a480',\n",
       "  'a481',\n",
       "  'a482',\n",
       "  'a483',\n",
       "  'a484',\n",
       "  'a485',\n",
       "  'a486',\n",
       "  'a487',\n",
       "  'a488',\n",
       "  'a489',\n",
       "  'a490',\n",
       "  'a491',\n",
       "  'a492',\n",
       "  'a493',\n",
       "  'a494',\n",
       "  'a495',\n",
       "  'a496',\n",
       "  'a497',\n",
       "  'a498',\n",
       "  'a499',\n",
       "  'a500',\n",
       "  'a501',\n",
       "  'a502',\n",
       "  'a503',\n",
       "  'a504',\n",
       "  'a505',\n",
       "  'a506',\n",
       "  'a507',\n",
       "  'a508',\n",
       "  'a509',\n",
       "  'a510',\n",
       "  'a511',\n",
       "  'a512',\n",
       "  'a513',\n",
       "  'a514',\n",
       "  'a515',\n",
       "  'a516',\n",
       "  'a517',\n",
       "  'a518',\n",
       "  'a519',\n",
       "  'a520',\n",
       "  'a521',\n",
       "  'a522',\n",
       "  'a523',\n",
       "  'a524',\n",
       "  'a525',\n",
       "  'a526',\n",
       "  'a527',\n",
       "  'a528',\n",
       "  'a529',\n",
       "  'a530',\n",
       "  'a531',\n",
       "  'a532',\n",
       "  'a533',\n",
       "  'a534',\n",
       "  'a535',\n",
       "  'a536',\n",
       "  'a537',\n",
       "  'a538',\n",
       "  'a539',\n",
       "  'a540',\n",
       "  'a541',\n",
       "  'a542',\n",
       "  'a543',\n",
       "  'a544',\n",
       "  'a545',\n",
       "  'a546',\n",
       "  'a547',\n",
       "  'a548',\n",
       "  'a549',\n",
       "  'a550',\n",
       "  'a551',\n",
       "  'a552',\n",
       "  'a553',\n",
       "  'a554',\n",
       "  'a555',\n",
       "  'a556',\n",
       "  'a557',\n",
       "  'a558',\n",
       "  'a559',\n",
       "  'a560',\n",
       "  'a561',\n",
       "  'a562',\n",
       "  'a563',\n",
       "  'a564',\n",
       "  'a565',\n",
       "  'a566',\n",
       "  'a567',\n",
       "  'a568',\n",
       "  'a569',\n",
       "  'a570',\n",
       "  'a571',\n",
       "  'a572',\n",
       "  'a573',\n",
       "  'a574',\n",
       "  'a575',\n",
       "  'a576',\n",
       "  'a577',\n",
       "  'a578',\n",
       "  'a579',\n",
       "  'a580',\n",
       "  'a581',\n",
       "  'a582',\n",
       "  'a583',\n",
       "  'a584',\n",
       "  'a585',\n",
       "  'a586',\n",
       "  'a587',\n",
       "  'a588',\n",
       "  'a589',\n",
       "  'a590',\n",
       "  'a591',\n",
       "  'a592',\n",
       "  'a593',\n",
       "  'a594',\n",
       "  'a595',\n",
       "  'a596',\n",
       "  'a597',\n",
       "  'a598',\n",
       "  'a599',\n",
       "  'a600',\n",
       "  'a601',\n",
       "  'a602',\n",
       "  'a603',\n",
       "  'a604',\n",
       "  'a605',\n",
       "  'a606',\n",
       "  'a607',\n",
       "  'a608',\n",
       "  'a609',\n",
       "  'a610',\n",
       "  'a611',\n",
       "  'a612',\n",
       "  'a613',\n",
       "  'a614',\n",
       "  'a615',\n",
       "  'a616',\n",
       "  'a617',\n",
       "  'a618',\n",
       "  'a619',\n",
       "  'a620',\n",
       "  'a621',\n",
       "  'a622',\n",
       "  'a623',\n",
       "  'a624',\n",
       "  'a625',\n",
       "  'a626',\n",
       "  'a627',\n",
       "  'a628',\n",
       "  'a629',\n",
       "  'a630',\n",
       "  'a631',\n",
       "  'a632',\n",
       "  'a633',\n",
       "  'a634',\n",
       "  'a635',\n",
       "  'a636',\n",
       "  'a637',\n",
       "  'a638',\n",
       "  'a639',\n",
       "  'a640',\n",
       "  'a641',\n",
       "  'a642',\n",
       "  'a643',\n",
       "  'a644',\n",
       "  'a645',\n",
       "  'a646',\n",
       "  'a647',\n",
       "  'a648',\n",
       "  'a649',\n",
       "  'a650',\n",
       "  'a651',\n",
       "  'a652',\n",
       "  'a653',\n",
       "  'a654',\n",
       "  'a655',\n",
       "  'a656',\n",
       "  'a657',\n",
       "  'a658',\n",
       "  'a659',\n",
       "  'a660',\n",
       "  'a661',\n",
       "  'a662',\n",
       "  'a663',\n",
       "  'a664',\n",
       "  'a665',\n",
       "  'a666',\n",
       "  'a667',\n",
       "  'a668',\n",
       "  'a669',\n",
       "  'a670',\n",
       "  'a671',\n",
       "  'a672',\n",
       "  'a673',\n",
       "  'a674',\n",
       "  'a675',\n",
       "  'a676',\n",
       "  'a677',\n",
       "  'a678',\n",
       "  'a679',\n",
       "  'a680',\n",
       "  'a681',\n",
       "  'a682',\n",
       "  'a683',\n",
       "  'a684',\n",
       "  'a685',\n",
       "  'a686',\n",
       "  'a687',\n",
       "  'a688',\n",
       "  'a689',\n",
       "  'a690',\n",
       "  'a691',\n",
       "  'a692',\n",
       "  'a693',\n",
       "  'a694',\n",
       "  'a695',\n",
       "  'a696',\n",
       "  'a697',\n",
       "  'a698',\n",
       "  'a699',\n",
       "  'a700',\n",
       "  'a701',\n",
       "  'a702',\n",
       "  'a703',\n",
       "  'a704',\n",
       "  'a705',\n",
       "  'a706',\n",
       "  'a707',\n",
       "  'a708',\n",
       "  'a709',\n",
       "  'a710',\n",
       "  'a711',\n",
       "  'a712',\n",
       "  'a713',\n",
       "  'a714',\n",
       "  'a715',\n",
       "  'a716',\n",
       "  'a717',\n",
       "  'a718',\n",
       "  'a719',\n",
       "  'a720',\n",
       "  'a721',\n",
       "  'a722',\n",
       "  'a723',\n",
       "  'a724',\n",
       "  'a725',\n",
       "  'a726',\n",
       "  'a727',\n",
       "  'a728',\n",
       "  'a729',\n",
       "  'a730',\n",
       "  'a731',\n",
       "  'a732',\n",
       "  'a733',\n",
       "  'a734',\n",
       "  'a735',\n",
       "  'a736',\n",
       "  'a737',\n",
       "  'a738',\n",
       "  'a739',\n",
       "  'a740',\n",
       "  'a741',\n",
       "  'a742',\n",
       "  'a743',\n",
       "  'a744',\n",
       "  'a745',\n",
       "  'a746',\n",
       "  'a747',\n",
       "  'a748',\n",
       "  'a749',\n",
       "  'a750',\n",
       "  'a751',\n",
       "  'a752',\n",
       "  'a753',\n",
       "  'a754',\n",
       "  'a755',\n",
       "  'a756',\n",
       "  'a757',\n",
       "  'a758',\n",
       "  'a759',\n",
       "  'a760',\n",
       "  'a761',\n",
       "  'a762',\n",
       "  'a763',\n",
       "  'a764',\n",
       "  'a765',\n",
       "  'a766',\n",
       "  'a767',\n",
       "  'a768',\n",
       "  'a769',\n",
       "  'a770',\n",
       "  'a771',\n",
       "  'a772',\n",
       "  'a773',\n",
       "  'a774',\n",
       "  'a775',\n",
       "  'a776',\n",
       "  'a777',\n",
       "  'a778',\n",
       "  'a779',\n",
       "  'a780',\n",
       "  'a781',\n",
       "  'a782',\n",
       "  'a783',\n",
       "  'a784',\n",
       "  'a785',\n",
       "  'a786',\n",
       "  'a787',\n",
       "  'a788',\n",
       "  'a789',\n",
       "  'a790',\n",
       "  'a791',\n",
       "  'a792',\n",
       "  'a793',\n",
       "  'a794',\n",
       "  'a795',\n",
       "  'a796',\n",
       "  'a797',\n",
       "  'a798',\n",
       "  'a799',\n",
       "  'a800',\n",
       "  'a801',\n",
       "  'a802',\n",
       "  'a803',\n",
       "  'a804',\n",
       "  'a805',\n",
       "  'a806',\n",
       "  'a807',\n",
       "  'a808',\n",
       "  'a809',\n",
       "  'a810',\n",
       "  'a811',\n",
       "  'a812',\n",
       "  'a813',\n",
       "  'a814',\n",
       "  'a815',\n",
       "  'a816',\n",
       "  'a817',\n",
       "  'a818',\n",
       "  'a819',\n",
       "  'a820',\n",
       "  'a821',\n",
       "  'a822',\n",
       "  'a823',\n",
       "  'a824',\n",
       "  'a825',\n",
       "  'a826',\n",
       "  'a827',\n",
       "  'a828',\n",
       "  'a829',\n",
       "  'a830',\n",
       "  'a831',\n",
       "  'a832',\n",
       "  'a833',\n",
       "  'a834',\n",
       "  'a835',\n",
       "  'a836',\n",
       "  'a837',\n",
       "  'a838',\n",
       "  'a839',\n",
       "  'a840',\n",
       "  'a841',\n",
       "  'a842',\n",
       "  'a843',\n",
       "  'a844',\n",
       "  'a845',\n",
       "  'a846',\n",
       "  'a847',\n",
       "  'a848',\n",
       "  'a849',\n",
       "  'a850',\n",
       "  'a851',\n",
       "  'a852',\n",
       "  'a853',\n",
       "  'a854',\n",
       "  'a855',\n",
       "  'a856',\n",
       "  'a857',\n",
       "  'a858',\n",
       "  'a859',\n",
       "  'a860',\n",
       "  'a861',\n",
       "  'a862',\n",
       "  'a863',\n",
       "  'a864',\n",
       "  'a865',\n",
       "  'a866',\n",
       "  'a867',\n",
       "  'a868',\n",
       "  'a869',\n",
       "  'a870',\n",
       "  'a871',\n",
       "  'a872',\n",
       "  'a873',\n",
       "  'a874',\n",
       "  'a875',\n",
       "  'a876',\n",
       "  'a877',\n",
       "  'a878',\n",
       "  'a879',\n",
       "  'a880',\n",
       "  'a881',\n",
       "  'a882',\n",
       "  'a883',\n",
       "  'a884',\n",
       "  'a885',\n",
       "  'a886',\n",
       "  'a887',\n",
       "  'a888',\n",
       "  'a889',\n",
       "  'a890',\n",
       "  'a891',\n",
       "  'a892',\n",
       "  'a893',\n",
       "  'a894',\n",
       "  'a895',\n",
       "  'a896',\n",
       "  'a897',\n",
       "  'a898',\n",
       "  'a899',\n",
       "  'a900',\n",
       "  'a901',\n",
       "  'a902',\n",
       "  'a903',\n",
       "  'a904',\n",
       "  'a905',\n",
       "  'a906',\n",
       "  'a907',\n",
       "  'a908',\n",
       "  'a909',\n",
       "  'a910',\n",
       "  'a911',\n",
       "  'a912',\n",
       "  'a913',\n",
       "  'a914',\n",
       "  'a915',\n",
       "  'a916',\n",
       "  'a917',\n",
       "  'a918',\n",
       "  'a919',\n",
       "  'a920',\n",
       "  'a921',\n",
       "  'a922',\n",
       "  'a923',\n",
       "  'a924',\n",
       "  'a925',\n",
       "  'a926',\n",
       "  'a927',\n",
       "  'a928',\n",
       "  'a929',\n",
       "  'a930',\n",
       "  'a931',\n",
       "  'a932',\n",
       "  'a933',\n",
       "  'a934',\n",
       "  'a935',\n",
       "  'a936',\n",
       "  'a937',\n",
       "  'a938',\n",
       "  'a939',\n",
       "  'a940',\n",
       "  'a941',\n",
       "  'a942',\n",
       "  'a943',\n",
       "  'a944',\n",
       "  'a945',\n",
       "  'a946',\n",
       "  'a947',\n",
       "  'a948',\n",
       "  'a949',\n",
       "  'a950',\n",
       "  'a951',\n",
       "  'a952',\n",
       "  'a953',\n",
       "  'a954',\n",
       "  'a955',\n",
       "  'a956',\n",
       "  'a957',\n",
       "  'a958',\n",
       "  'a959',\n",
       "  'a960',\n",
       "  'a961',\n",
       "  'a962',\n",
       "  'a963',\n",
       "  'a964',\n",
       "  'a965',\n",
       "  'a966',\n",
       "  'a967',\n",
       "  'a968',\n",
       "  'a969',\n",
       "  'a970',\n",
       "  'a971',\n",
       "  'a972',\n",
       "  'a973',\n",
       "  'a974',\n",
       "  'a975',\n",
       "  'a976',\n",
       "  'a977',\n",
       "  'a978',\n",
       "  'a979',\n",
       "  'a980',\n",
       "  'a981',\n",
       "  'a982',\n",
       "  'a983',\n",
       "  'a984',\n",
       "  'a985',\n",
       "  'a986',\n",
       "  'a987',\n",
       "  'a988',\n",
       "  'a989',\n",
       "  'a990',\n",
       "  'a991',\n",
       "  'a992',\n",
       "  'a993',\n",
       "  'a994',\n",
       "  'a995',\n",
       "  'a996',\n",
       "  'a997',\n",
       "  'a998',\n",
       "  'a999',\n",
       "  ...],\n",
       " 'target_names': ['class'],\n",
       " 'DESCR': '**Author**: Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton    \\n**Source**: [University of Toronto](https://www.cs.toronto.edu/~kriz/cifar.html) - 2009  \\n**Please cite**: Alex Krizhevsky (2009) Learning Multiple Layers of Features from Tiny Images, Tech Report.\\n\\n**CIFAR-10 small**  \\nThis is a 20,000 instance sample of the original CIFAR-10 dataset. Sampled randomly and stratified, with 2000 examples per class. Training and test set are merged. Find the corresponding task for the original train-test splits.\\n\\nCIFAR-10 is a labeled subset of the [80 million tiny images dataset](http://groups.csail.mit.edu/vision/TinyImages/). It (originally) consists 32x32 color images representing 10 classes of objects:  \\n0. airplane  \\n1. automobile          \\n2. bird          \\n3. cat          \\n4. deer          \\n5. dog          \\n6. frog          \\n7. horse          \\n8. ship          \\n9. truck          \\n\\nThe classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\\n\\nThe original CIFAR-10 dataset contains 6000 images per class. The original train-test split randomly divided these into 5000 train and 1000 test images per class.\\n\\n### Attribute description  \\n\\nEach instance represents a 32x32 colour image as a 3072-value array. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\\n\\nThe labels are encoded as integers in the range 0-9, corresponding to the numbered classes listed above\\n\\nDownloaded from openml.org.',\n",
       " 'details': {'id': '40926',\n",
       "  'name': 'CIFAR_10_small',\n",
       "  'version': '1',\n",
       "  'description_version': '1',\n",
       "  'format': 'ARFF',\n",
       "  'upload_date': '2017-09-26T23:24:38',\n",
       "  'licence': 'Public',\n",
       "  'url': 'https://api.openml.org/data/v1/download/16797612/CIFAR_10_small.arff',\n",
       "  'parquet_url': 'http://openml1.win.tue.nl/dataset40926/dataset_40926.pq',\n",
       "  'file_id': '16797612',\n",
       "  'default_target_attribute': 'class',\n",
       "  'tag': 'derived',\n",
       "  'visibility': 'public',\n",
       "  'minio_url': 'http://openml1.win.tue.nl/dataset40926/dataset_40926.pq',\n",
       "  'status': 'active',\n",
       "  'processing_date': '2018-10-04 07:18:53',\n",
       "  'md5_checksum': '060250588efa7a126a72ab523c8f824c'},\n",
       " 'url': 'https://www.openml.org/d/40926'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem 2\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "#Use the fetch openml command from sklearn.datasets to import the CIFAR-10-Small data set\n",
    "\n",
    "df = fetch_openml('CIFAR_10_small')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACiCAYAAADC8hYbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAch0lEQVR4nO1da2wU173/z2Nnd71Pv9bGLwxJIOQBaR0MJrm9eaByqZSGkg/pp9KmakRrVyJ8qETVpGrUylW/lKai9EsDrXQRFdElbdNbUl0IoKTQBDfcGyCQkJBg8AMbe+19zO7szJz7wWb+53/AwBITD/j8pJXm7H/2zJnZ/8z5zf91FMYYAwmJGYY60wOQkACQiijhE0hFlPAFpCJK+AJSESV8AamIEr6AVEQJX0AqooQvIBVRwheQiijhC9w0RdyyZQu0trZCKBSCZcuWwdtvv32zDiVxG0C5Gb7mP/7xj/CNb3wDfvvb38KyZctg8+bNsGvXLjh16hSkUqmr/tZ1Xejr64NYLAaKokz30CQ+ZzDGIJPJQENDA6jqVZ577Cagvb2ddXZ2em3HcVhDQwPr7u6+5m97e3sZAMjPbfbp7e296v+uwzTDsizo6emBTZs2ed+pqgorV66EQ4cOXbZ/sViEYrHotdnkA7qtfRno+sTwxsZGPXlQdcnvKw18oDdVVhBZTRVtVyci3rahBohMC4bpwDTN2xxNjxFRycZjJhMJIlOdEmkXLTy3QqFIZKFwkLQdcLxt08wRWTwRwwZziMyy6DE1wL9V484DACAaiZJ2pAKvkR4IEVmhaOEhFeFpplLVsSzc12Y4kxWKFjz/0n9CLBaDq2HaFXF4eBgcx4G6ujryfV1dHZw8efKy/bu7u+EnP/nJ5QPTdU8R+YupqXS61jVUCiNAL3owQE8vZKDyGRpVRD1I26Dhb02DylQVjxkSfqdSHQEFuBvHpcKQ0K/DUXbXEcbOH4dRpVCBsisNcF9REcPCeMMhw9sOBAwi45nRtRRR4/blFRH7ujrNmnZFLBebNm2CjRs3eu3x8XFobm6GkyffB2WSU6SHhz15Fb1pQanGL2ocetcpYcpHc+6It5116J/HFPon5At4h+dN+iQrOahcwxq9wCGd9mvbuK8m/HnBIH0i5gv4FLRdi8iUQrW3rVLdglKRji+s4zXJFmk/I45N2hUVOEsowiyh8DerwO/yBfoUtkvY1nQ8r2KJHm8qTLsi1tTUgKZpMDg4SL4fHByE+vr6y/YPBoOX/SESsw/Tbr4xDAPa2tpg79693neu68LevXuho6Njug8ncZvgpkzNGzduhHXr1sGDDz4I7e3tsHnzZsjlcvCtb33ruvsI6Qqol/gg98CcW03n5tY6fFlI1VYRWZibdgAoTzGLBSIrlOj0xrh9jbDwIsO9rDCX/i4hvCDZJZ7D0n4cgU9qBjelWXR8JRvHU2HQGUSP0H5DnNxW6EuPyujLng3Yr8AyIBrBc8nm8sJ46NTMU/fMOL7cWSXhJKfATVHEp59+GoaGhuCFF16AgYEBeOCBB2DPnj2XvcBISFzCTXtZ6erqgq6urpvVvcRtBulrlvAFZtx8MxVCigOqMsFnYjEc5oLGSrJfdRhtGQGX8qrsCDVdOC7ed2aemhVUar2BeBINv7rAydJjGZQJV7AqRjliZhw5mlWgfM0UTCCM42vRCOW3JcvEsQo2xoBgdXA4o7ouEL9ikR7T4GyHqkuvSTGLjgQQzF1BwYRku8g9x3LImy2bctKpIJ+IEr6AVEQJX8C3U3MyqIE2ac0Pc1NPQjBV1MbR+u8ILjTRcKDp3HwieAqKLp2ydG7O1QWTh1PEaZJptJ8LF9J0X858kclTE0jeodQhGo5zA6Kj1zhXoaoILr0gNWmZOaQoFYE4kelCsFWB8yCZghfE5VyH6SylPek8vV5ZjuoUSnhNbEdOzRK3EKQiSvgCUhElfAHfcsSaRAj0Sf4V48K7QiFqN1C5MLCw4Ior2ZRnuZx5hDHKzyybcieHi/FzmWBm4bgd06ndJ2NRE43j4HjzAl8S+VMmh8c5P0L7CXBxmPEsNcmUBoZJ2xxDLtpScyeRpVJNpK3E0B1XHL1IZNksjmEsQzni8JhJ2p/0Yj8OF0LnXmcCgHwiSvgCUhElfAGpiBK+gG85Yn1NBRiTdr+4gTaqaIUQzk74GxNklIMVTeROKlCeVR2juSeRCNrmxscoB0vE0TaXEdx0n56n+2aLyBENwaTWWEEvvx5A3vXJxTQdO+NcmYIdMRGnkekr7nkQx95PeTLLC7+tQTtsMU/Hk83icyoYoNHbzfX0mKkURlYNjiOftB0Xzh47B9eCfCJK+AJSESV8Ad9OzZXRMAQnzTa6lfa+FzPzKoIY7VI06TRZEqJJkkmM3GGCWcFy6D1ZKnFusihNwewbwuiSjz6lqaZDGXpMPshnbpiantb82wOk3TQHj/NKz8dEduj0gLctJlbpKj2XTHoIj5+lEeSxmJCt6CBFCYWozOBMZRUKldlCElZLcwMeYwSjk6ySAwfl1Cxxq0AqooQvIBVRwhfwLUesrayCkDExPHME+ZqqCCYGLhzJtChv0RXKyfJcSJZ4B5olyruSlWiisYTo5I/P9XnbI+OCeURw+WlcmFg8RPdN6RnSDo0gn7srTnPA+6uwn8H0BSIr5unY3/3gA29bFSKkSxEaFgYJLqFNKACQSCD/jrlC+JhQ5oRZ4952a22E2+/6EuzlE1HCF5CKKOELSEWU8AV8yxGT1TVe5arKKIZ3qUKhoPQ4ZpqVclkiUx0xDAz5EhPskdEoDbcvAbbf//gDIssVMTwqFKIZdJd47SWEuWoJlRrlSz2naX0g28LfFhOUI9ZW4ngUoDyvZNMQrTyX8ZcTXHqWTceg8NxYqPQQ4Mo3MKHyU0BIX7T50oIcp2aODAOTuIVQtiIePHgQnnjiCWhoaABFUeDVV18lcsYYvPDCCzBnzhwIh8OwcuVK+PDDD6drvBK3KcqemnO5HCxZsgSeeeYZWLt27WXyX/ziF/DSSy/B73//e5g3bx48//zzsGrVKjhx4gSEQqEr9DgFVB1gchpWhMgPHkHOLVUBNCldF+4zvoZzCahZIxim0TfDA2hayQ+PEtl8rkijUMsJQhGaYL/wjkY8vrCzLRQLHedohq5R12HMwHOrrryDyO64q4W0z5x9x9s++cF5IjN0odgUQzpj21QdVM4UFRCKirouvX589LvCFfVUxAKfU6BsRVy9ejWsXr36ijLGGGzevBl+9KMfwZNPPgkAAH/4wx+grq4OXn31Vfj6179e7uEkZgmmlSOeOXMGBgYGYOXKld53iUQCli1bdsX62QATNbTHx8fJR2L2YVoVcWBgIkLkSvWzL8lEdHd3QyKR8D7Nzc3TOSSJWwQzbr6ZqoZ2oWADTBYFV0p8xhg1P+Ry+AS1SvS+slXKSbN55H3jeepea2yml4LZKJ9bQ+0adzQgX8oXqKxxwRLSNhjywtEx6hYLJ6tJGy6iiaS5fg4RpXNoMpp/911EFhdWU4hXLsJjDtHzHB2j3DPAcU+VUVNUiaucIVBCcISqEHyhTj7ETgy3mwrT+kS8VCP7eutnA0zU0I7H4+QjMfswrYo4b948qK+vJ/Wzx8fH4Z///Kesny1xVZQ9NWezWTh9+rTXPnPmDBw9ehSqqqqgpaUFNmzYAD/96U/hrrvu8sw3DQ0NsGbNmrKO4ygOOJOv/oyLBhYf9eEQel2iQm3CviGaBH7mHEYu6wHajzHYR9qFQdz3rhQ1XTz+CE6NH50fIbJYYy1p11TjTHBhiM4UySQ1N6kutw6M4Mm4MIRmGD2UJrKhdD9pn+9Hk0wgQK9JMk7nWNPkplGdPpcUbr51hQJXqrBuisKZxq7TmUJQtiIeOXIEHn30Ua99id+tW7cOtm/fDj/4wQ8gl8vBs88+C+l0Gh5++GHYs2dPeTZEiVmHshXxkUceuSoBVRQFXnzxRXjxxRc/08AkZhekr1nCF5hx881USCQi3jpxto4cMSsUjGRc1PVYhpomPj1LOVk2i9wpHKL3YP8Zakiv49aoa2ycS2TJhnnediAj2DWETLimJe0oGqDutrA9RNoO4LnlcvQ851Qg97SE4k2KsNBjU4TLqEtSa0XmIrXnXhjEwkslIVOvwC1oCUKmYEQoDmqZHC/l3IGOGNIzBeQTUcIXkIoo4QtIRZTwBXzLEbNjI2AXJriGbqGbKiCGFXHmNl1YmzifpZyxMoZ2u2SEchxzlHLEVAO63xoX/zuRHTuHUc0fnKYZdCvm0PUA02mU191B3X8q0OLuVhE5Y1IoIDV+AblcWMigm1MlHNNBV11gMV2XxhRsjm/995+97XO9lLNqJPSLcj1TMJyUuGeayi2ZW7jOZXLlE1HCF5CKKOEL+HZqVhVcttXhTANMmCJULhrHERLqR+kMBuPjnDtLWNl9ToK625Zy3qOmhcuJ7L+2vext1wumE82ibsXzH3+E+86/h8hC1bS+dYRxUeEjNIk+7OIUa5l0Sh/O0HayFs1L1fWtRGZmaVCJyjUdg5qMeBdfSShAoAj1yRWGbT7SuySTpyRuJUhFlPAFpCJK+AK+5YgKm/gAADicOUAR1tDjI5eYUKhTEbxvVdUYElVfQc0KX3xwAWkvWoG8cPQCTdwP2mgWmt9E1y1xhYPWp9A1ZxeEIp5pca0XlJdM+tc4gFz0o/O08OV7x46Q9orl2G91PY0CH89Q7slHidW0Up7s8qFdFuWEtsCxx4bS3nYxwxVPLYkrIl4Z8oko4QtIRZTwBaQiSvgCvuWIru2AO1nk0iwi7zIEu52uoxtKUylvubOeurdCYbzvWufStNUlDz9K2nMWLva2jx7aRmQtzdhv/b33E5lRS6sw6BVYQSJfoFzTHKcZdoN9vd726CDlgU4JbYXhGHVP1tTQ8K3evne97bo5jURm5+kYmImhXkqOVrRwGLcutbC2y6UCWZdg1GN7PIj2x4Ilw8AkbiFIRZTwBXw7NQc0HQKTy62Oci4sR0hoD1dgFp8mRBGnqmkGW29/2tu+44v/QWRN99M2AE6/pQxdsjbBLZdWu+ABIsvpNBLm+LtYEKlo0n7Gx9OkPXz+rLetOZRmhEL4VzXOo9Pt4gXUVWhraIYJaEkiCxjUxKUX0K2X/5RGkLucG88WHllZIdKpohqPWcdFLpkFab6RuIUgFVHCF5CKKOEL+JYjWoUiqJPVBSqCOEwlJNRyVrkqEML6cOEo3ferT3/V216x+nEii9fQCmaDH7/vbWsq7TfNZQsOfXKKyPoylBPt5yrqRsNCllyRmlLq65B7xmPU3XbmHJp2LGE8VQ2tpL3g/jZsOLSw0kiamoX4IlKjplBfm+F1L5jUdZkVctsZl125KInfFyjVnRLyiSjhC5SliN3d3bB06VKIxWKQSqVgzZo1cOoUfSIUCgXo7OyE6upqiEaj8NRTT11WHUxCQkRZU/OBAwegs7MTli5dCrZtww9/+EP48pe/DCdOnIBIZGIqee655+Cvf/0r7Nq1CxKJBHR1dcHatWvhrbfeKmtgLrPAvZRAxBUAUoQlvWxuBXtFsP6HgjQa+YE2nLLEFdlPHH2XtEf7MLK6KNS+zoxi4aXe0yeILMvCpB1w8LdRnVKFeIhOv7WVODX3D9JEeJuLQMpn6JTee+YsUBzH8WSFZdZ0eo3sYMrbvmjT6xUOowenIkbPK6zTKT+Tx+Qzm1ue2Havz3xTliLu2bOHtLdv3w6pVAp6enrgS1/6EoyNjcHvfvc72LFjBzz22GMAALBt2zZYtGgRHD58GJYvX36lbiUkPhtHHJusPlo1mc7Y09MDpVKJ1NC+++67oaWlRdbQlrgqblgRXdeFDRs2wEMPPQT33XcfAEzU0DYMA5LJJNlX1tCWuBZu2HzT2dkJx44dgzfffPMzDWCqGtoA7uQHwLXRBqALhScdzg1lCfW16xI0+ub1P7/mbVfVHSey1Bx6A1h5NNEEApQPRbmlZnWhoGZE4J71Kc7dlaHRLWGN9ntxaNjbLgkR0TGuIKmVpRzxw3dphHb/SVyyrWjTrEII0PE63PgjTZSzQgSvuxqkPDnk0mtdCTi+RfdiFmHeLAHA/8K1cEOK2NXVBa+99hocPHgQmrhQ+fr6erAsC9LpNHkqXquGdjAYvKJMYvagrKmZMQZdXV2we/du2LdvH8ybN4/I29raIBAIkBrap06dgrNnz8oa2hJXRVlPxM7OTtixYwf86U9/glgs5vG+RCIB4XAYEokEfPvb34aNGzdCVVUVxONx+P73vw8dHR3yjVniqihLEbdu3QoAE+WLeWzbtg2++c1vAgDAL3/5S1BVFZ566ikoFouwatUq+M1vflP2wFxXAdedcD8ZnP0tpAupefxSrpqQhSYUKxoexhem7BB9eQqX6Nu6y1V3qqqkmXDJBi4zz6Fr253vo/0yQLudqtLLLS5Zq3GFMiMhyoV586km2FJBsJ86FvJb1aVhc+N5ylOtIHLIWAM9l1w47W1nXOqrK+ToZFodn+9t13C8OJe7Ph9fWYp4PYu3hEIh2LJlC2zZsqWcriVmOaSvWcIX8G30jaoEQVUmhhcKommACSaaSBinsEishsjyJWpyqI5hXWxd6Mcao/5wV8V98wE6FdbV4Uuaa9GpZ+FimnD/jzfwxc1itFhSQFirxMyiPB6j7jaDWzFeE5L4swV6nmf6cfpNp+l5FhUaJV67AJ9FjUnqxrMYXoPRYTp2o0DNVJFGzkyVR9OTacoIbYlbCFIRJXwBqYgSvoBvOWJAV8CYrLCUL6JZQRNCp1zOTZYvUXeWJqy3FzSQAwUCtB+DS4QHAEjEUT4grKGXb0QemGqmGXTnLwyT9r1LH/K2s0N0vb+PP6Buxlw27W3rGj2XRAI5owKUI/afp/2e/ZQz3wTpecbrqFmotorrV+Caygj+tnKUqkpjimYrNiXxmpw+gSYssyBUS50C8oko4QtIRZTwBaQiSvgCvuWIqWoVKibXyytdxDVGTGEduhxnFmMqtVnpOj29eBxtXYYQrmXmqIsvHOB+a9F+jvzjH972/IWUP547R118KueCrBAKF2lCGFg4jJwsl6Uc0TSxbdvUdhkN035WfAGLjoYEe6StUbsiX9zJ7KUcUc1gqkCqIkZkX1hwL2mnkpgF2dN/xtsuWHKdFYlbCFIRJXwB307NTU2Gl5CeUHCKON1LXU2DQ2iisYRk8miUnl6Oi7p2XBrlrAn35MgQ0oFMlk4vhRL2ozG6zFosSqPCBwcw4++csPSty6iLr64WqYPiUrPHaBrddsEIPc9kgk6bhobnUhQivUGn9CBXxH2trOC2c1F2ZzMNbG4QanP3nkOKcnEI/yNZQ1viloJURAlfQCqihC/gW44YTwYgWjHBWUyOc1SmaBYaRNBlNTxII4wLQoiWbqApQxCBK3CZEhd5PWbSqOYIZy4p5CnvMwvUxWdx/TrCMRij55Id58LA4jQkKx5HF6QprsV3kY4vGkUzkLgujWJTt6eh43GCtDQ3GAaOr/XOViIz87Sfgwex4sX/fYBrudiCuW0qyCeihC8gFVHCF/Dt1KyFdNAn60aH4hgpXBUVlkDjlmcIhIVV34WIEXDwt+FQioqEKGynmPa2jQraT0DH8WgajWYpCivPW9zyskww1wg5T8AsnOYdOuNDgDe7GNR8kx6lU7PJJY0lktSzogtTtcqdS16IWh8cxgJOo4IJK5OjZqv/2X8Sf8cxB9eVy+RK3EKQiijhC0hFlPAFfMsRc1kdFHeSF2m47Fk0QslTIIwcJCLYHxIJIdtt3OS2adRMNi+Yb7j1QWIGdWeFuMgdu0hNRrpO722DawaC1FyjKHTfCs4lKeTig83VBzfCQlRRkvLUkRHkdhmBs8ar6LnkuUieDz+5SGQn38O63XVVlGvWNdFjgorHqeFcjo7rwqejAuG9AuQTUcIXKEsRt27dCosXL4Z4PA7xeBw6Ojrgb3/7myeX9bMlbhRlKWJTUxP8/Oc/h56eHjhy5Ag89thj8OSTT8Lx4xNJQM899xz85S9/gV27dsGBAwegr68P1q5de1MGLnF7oSyO+MQTT5D2z372M9i6dSscPnwYmpqaprV+dl8vQMUk5SumkfvFaqk9KxTmbGZ0BV2oqqKnl82hgSudpm6y0YuG0MZtzaXczuVqADmOEOYkFC/n73RFpXZETYggNzk7JxMCmwNcWJidHyEyR3D5OZzNMZ2lMjEqbITjzZ+cphwxfRHD360c/WF9goaFLZqL6wNyXULJceFfn9DxXgk3zBEdx4GdO3dCLpeDjo6OG6qfDSBraEtMoGxFfO+99yAajUIwGIT169fD7t274Z577rmh+tkAsoa2xATKNt8sXLgQjh49CmNjY/DKK6/AunXr4MCBAzc8gKlqaDuBanAma1eXjAc9edGl5hLVxmiXUIJOfclaas6p5JYOq8pTs0Z6hEa7pIdxOjZz9DI5NjeNM3ovu0LtwoKJpgvDoNO/Jqy7kingb82sYKZiaGaJqTQi21XpLFIq4XiDEWHtGaEeeNLAfudDksjuX4JRPAsXLyGy1jtpYYH25UgBzvVh9HvRsgH+9QlcC2UromEYcOfkINra2uCdd96BX/3qV/D000+XXT8bQNbQlpjAZ7Yjuq4LxWJR1s+W+Ewo64m4adMmWL16NbS0tEAmk4EdO3bA/v374fXXX5+2+tmXqtLmuWUtTW5bCdCkIpdbZkHN06lZzwl1V7i855yw2mbOpPvm+WmyQKc3urLDNabmIvbrCNE3mvDGbRbxOAWh7DLjlnrThdVJxdzhIt8UQnw0IRi3WMKdLZuOJ8DJ8sIyo1khEczkzrPIjedS/9esNszKwDPPPMPmzp3LDMNgtbW17PHHH2d///vfPblpmux73/seq6ysZBUVFexrX/sa6+/vL+cQrLe3lwGA/Nxmn97e3qv+7wq7nsLYnyNc14W+vj5gjEFLSwv09vZCPB6/9g9nGS691Pn9+jDGIJPJQENDA6jq1EzQd0EPqqpCU1OTZ0+85E6UuDJuheuTSCSuuY8MepDwBaQiSvgCvlXEYDAIP/7xj6WNcQrcbtfHdy8rErMTvn0iSswuSEWU8AWkIkr4AlIRJXwBqYgSvoBvFXHLli3Q2toKoVAIli1bBm+//fZMD2lG0N3dDUuXLoVYLAapVArWrFkDp06dIvvcFklrZUUkfE7YuXMnMwyDvfzyy+z48ePsO9/5Dksmk2xwcHCmh/a5Y9WqVWzbtm3s2LFj7OjRo+wrX/kKa2lpYdls1ttn/fr1rLm5me3du5cdOXKELV++nK1YsWIGR10+fKmI7e3trLOz02s7jsMaGhpYd3f3DI7KH7hw4QIDAHbgwAHGGGPpdJoFAgG2a9cub5/333+fAQA7dOjQTA2zbPhuarYsC3p6ekgSlqqqsHLlyqsmYc0WjI1NVOGqqppYC+9Gk9b8Bt8p4vDwMDiOA3V1deT7ayVhzQa4rgsbNmyAhx56CO677z4AgBtOWvMbfBcGJjE1Ojs74dixY/Dmm2/O9FCmHb57ItbU1ICmaZe99V0rCet2R1dXF7z22mvwxhtvQFMTLklbX1/vJa3xuNWul+8U0TAMaGtrI0lYruvC3r17Z2USFmMMurq6YPfu3bBv3z6YN28ekd82SWsz/bZ0JezcuZMFg0G2fft2duLECfbss8+yZDLJBgYGZnponzu++93vskQiwfbv38/6+/u9Tz6f9/ZZv349a2lpYfv27WNHjhxhHR0drKOjYwZHXT58qYiMMfbrX/+atbS0MMMwWHt7Ozt8+PBMD2lGAFMkI23bts3bZzqS1mYaMh5RwhfwHUeUmJ2QiijhC0hFlPAFpCJK+AJSESV8AamIEr6AVEQJX0AqooQvIBVRwheQiijhC0hFlPAF/h9l2m/8YaDsUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Figure out how to display some of the images in this data set, and display a couple.\n",
    "\n",
    "\n",
    "#Plot Images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "index = 0\n",
    "display_image = df.data\n",
    "display_image = display_image.iloc[[index] , :]\n",
    "#display_label = df.target\n",
    "display_image = display_image.to_numpy().reshape(3,32,32).T /255\n",
    "display_image = display_image.swapaxes(0,1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "print(display_image.shape)\n",
    "plt.figure(figsize=(1.5,1.5))\n",
    "plt.imshow(display_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split for x train y train x test y test on a 3/4 / 1/4 ratio\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.data.to_numpy()\n",
    "y = df.target.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=1/4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Logistic Regression using multinomial.\n",
    "\n",
    "#Stadard scaler as preprocessing step for solver sage\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression using multinomial\n",
    "\n",
    "#c_val=[x for x in np.arange(0.1,0.3,0.1)]\n",
    "c_val=[0.1,0.2,0.4,0.8,1,1.2,1.5,1.8,2]\n",
    "\n",
    "l1_res={}\n",
    "l1_loss={}\n",
    "#L1 regularization\n",
    "for c in c_val:\n",
    "    logisticRegression = LogisticRegression(solver='saga',multi_class='multinomial',C=c,penalty='l1')\n",
    "    logisticRegression.fit(X_train_scaled,y_train)\n",
    "    #y_pred=logisticRegression.predict(X_test_scaled)\n",
    "    l1_res[c]=logisticRegression.score(X_test_scaled,y_test)\n",
    "    l1_loss[c]=log_loss(y_test,logisticRegression.predict_proba(X_test),labels=['0','1','2','3','4','5','6','7','8','9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest accuracy was achieved for C value of:  0.1  for an accuracy of:  0.3924\n",
      "loss values for each value of alpha: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.1: 23.53744195915302,\n",
       " 0.2: 23.362152180886717,\n",
       " 0.4: 23.36308947315987,\n",
       " 0.8: 23.405343563731087,\n",
       " 1: 23.418739132240937,\n",
       " 1.2: 23.423504606880165,\n",
       " 1.5: 23.429965226374346,\n",
       " 1.8: 23.43889439491987,\n",
       " 2: 23.44063944538978}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Report your training and test loss from above\n",
    "\n",
    "lowest = max(l1_res, key=l1_res.get)\n",
    "\n",
    "\n",
    "print(\"Highest accuracy was achieved for C value of: \", lowest,\" for an accuracy of: \",l1_res[lowest])\n",
    "print(\"loss values for each value of alpha: \\n\")\n",
    "\n",
    "l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/amanbhardwaj/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "l2_res={}\n",
    "l2_loss={}\n",
    "#L1 regularization\n",
    "for c in c_val:\n",
    "    logisticRegression = LogisticRegression(solver='saga',multi_class='multinomial',C=c,penalty='l2')\n",
    "    logisticRegression.fit(X_train_scaled,y_train)\n",
    "    y_pred=logisticRegression.predict(X_test_scaled)\n",
    "    l2_res[c]=logisticRegression.score(X_test_scaled,y_test)\n",
    "    #l2_loss[c]=log_loss(y_test,y_pred)\n",
    "    l2_loss[c]=log_loss(y_test,logisticRegression.predict_proba(X_test),labels=['0','1','2','3','4','5','6','7','8','9'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest accuracy was achieved for C value of:  0.1  for an accuracy of:  0.3606\n",
      "loss values for each value of alpha: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.1: 23.447004521863878,\n",
       " 0.2: 23.458422416011793,\n",
       " 0.4: 23.46595484566457,\n",
       " 0.8: 23.471204242837615,\n",
       " 1: 23.471073068087858,\n",
       " 1.2: 23.467118304244643,\n",
       " 1.5: 23.46962229388,\n",
       " 1.8: 23.4703788468968,\n",
       " 2: 23.47565165956086}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest = max(l2_res, key=l2_res.get)\n",
    "\n",
    "\n",
    "print(\"Highest accuracy was achieved for C value of: \", lowest,\" for an accuracy of: \",l2_res[lowest])\n",
    "\n",
    "print(\"loss values for each value of alpha: \\n\")\n",
    "\n",
    "l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01\n",
      "Sparsity with L1 penalty:                95.94%\n",
      "Sparsity with L2 penalty:                0.00%\n",
      "Score with L1 penalty:                   0.38\n",
      "Score with L2 penalty:                   0.39\n",
      "C=0.04\n",
      "Sparsity with L1 penalty:                88.01%\n",
      "Sparsity with L2 penalty:                0.00%\n",
      "Score with L1 penalty:                   0.40\n",
      "Score with L2 penalty:                   0.37\n",
      "C=0.06\n",
      "Sparsity with L1 penalty:                83.67%\n",
      "Sparsity with L2 penalty:                0.00%\n",
      "Score with L1 penalty:                   0.40\n",
      "Score with L2 penalty:                   0.37\n",
      "C=0.08\n",
      "Sparsity with L1 penalty:                73.85%\n",
      "Sparsity with L2 penalty:                0.00%\n",
      "Score with L1 penalty:                   0.40\n",
      "Score with L2 penalty:                   0.37\n"
     ]
    }
   ],
   "source": [
    "#How sparse can you make your solutions without deteriorating your testing error too much?\n",
    "#Here, we ask for a sparse solution that has test accuracy that is close to the best solution you\n",
    "#found.\n",
    "\n",
    "\n",
    "l1_spar={}\n",
    "l2_spar={}\n",
    "#we also tested with original c vals and found that sparsity decreased the higher our c values were\n",
    "#for this part we lowered the c values further\n",
    "\n",
    "\n",
    "c_val=[0.01,0.04,0.06,0.08]\n",
    "# Set regularization parameter\n",
    "for C in c_val:\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty=\"l1\", tol=0.01, solver=\"saga\")\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty=\"l2\", tol=0.01, solver=\"saga\")\n",
    "\n",
    "\n",
    "    clf_l1_LR.fit(X_train_scaled,y_train)\n",
    "    clf_l2_LR.fit(X_train_scaled,y_train)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L1 penalty:\", sparsity_l1_LR))\n",
    "\n",
    "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L2 penalty:\", sparsity_l2_LR))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Score with L1 penalty:\", clf_l1_LR.score(X_test_scaled,y_test)))\n",
    "\n",
    "    print(\"{:<40} {:.2f}\".format(\"Score with L2 penalty:\", clf_l2_LR.score(X_test_scaled,y_test)))\n",
    "\n",
    "    l1_spar[C]=sparsity_l1_LR\n",
    "    l2_spar[C]=sparsity_l2_LR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We got a sparsity of 88% with highest accuracy of 40% as our best result at c=0.04 that beats our previous accuracy of 39% at c=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=150)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=150)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem 4 - Part 1 - CIFAR : What is the best accuracy you can get on the test data, by tuning Random Forests?\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = fetch_openml('CIFAR_10')\n",
    "X = df.data.to_numpy()\n",
    "y = df.target.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=1/4, random_state=0)\n",
    "\n",
    "\n",
    "rf_cifar = RandomForestClassifier(n_estimators=150)\n",
    "rf_cifar.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for Random Forest Classifier is:  46.355555555555554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pred=rf_cifar.predict(X_test)\n",
    "\n",
    "score = cross_val_score(rf_cifar, X_train, y_train)\n",
    "print (\"Cross Validation Score for Random Forest Classifier is: \",np.mean(score)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x3859bf970>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem 4 - Part 2 - What is the best accuracy you can get on the test data, by tuning any model including Gradient boosting?\n",
    "\n",
    "import catboost as ctb\n",
    "\n",
    "#hyperparameters for best catboost model\n",
    "cat = ctb.CatBoostClassifier(iterations=1000,\n",
    "                             learning_rate=0.03, depth=6, loss_function='MultiClass',\n",
    "                             early_stopping_rounds= 1000, verbose=0)\n",
    "\n",
    "\n",
    "cat.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 51.73%\n",
      "Cross Validation score for CatBoost Classifier:  50.38222222222222\n"
     ]
    }
   ],
   "source": [
    "y_pred = cat.predict(X_test)\n",
    "#catboost accuracy score\n",
    "score = (accuracy_score(y_test, y_pred)) * 100\n",
    "print(\"Accuracy score is %.2f%%\" % score)\n",
    "#cross validation score\n",
    "cv_results = cross_val_score(cat, X_train, y_train,\n",
    "                             cv=2, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "\n",
    "print(\"Cross Validation score for CatBoost Classifier: \", np.mean(cv_results) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
